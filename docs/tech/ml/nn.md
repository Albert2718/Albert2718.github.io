参考：<https://zhuanlan.zhihu.com/p/364620596>

## 激活函数

激活函数（Activation Function）是一种添加到人工神经网络中的函数，旨在帮助网络学习数据中的复杂模式。在神经元中，输入的input经过一系列加权求和后作用于另一个函数，这个函数就是这里的激活函数。

### Why？

如果没有激活函数的话，不管有多少层神经网络都等价于一个线性变换。而纯粹的线性变换并不能够解决一些复杂的问题。

而引入激活函数之后，我们会发现常见的激活函数都是非线性的，因此也会给神经元引入非线性元素，使得神经网络可以逼近其他的任何非线性函数，这样可以使得神经网络应用到更多非线性模型中。

### Sigmoid 函数

Sigmoid函数也叫Logistic函数，用于隐层神经元输出，取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好。sigmoid是一个十分常见的激活函数，函数的表达式如下：
$$
S(x) = \frac{1}{1 + e^{-x}} 
$$

优点：平滑连续，有明确的输出范围(0,1)，适合概率预测。

不足：饱和神经元梯度消失问题、计算量大。

### ReLU 函数

ReLU函数又称为修正线性单元（Rectified Linear Unit），是一种分段线性函数，其弥补了sigmoid函数以及tanh函数的梯度消失问题，在目前的深度神经网络中被广泛使用。ReLU函数本质上是一个斜坡（ramp）函数，表达式如下：
$$
f(x) = max(0, x)
$$

优点：输入为正时，导数为1，一定程度上改善了梯度消失问题，加速梯度下降的收敛速度；计算简单，节省计算资源。*被认为具有生物学合理性（Biological Plausibility）,比如单侧抑制、宽兴奋边界（即兴奋程度可以非常高*

不足：Dead ReLU问题；不以0为中心

### Softmax 函数

Softmax 是用于多类分类问题的激活函数，在多类分类问题中，超过两个类标签则需要类成员关系。对于长度为 K 的任意实向量，Softmax 可以将其压缩为长度为 K，值在（0，1）范围内，并且向量中元素的总和为 1 的实向量。函数表达式如下：

$$
S(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{K} e^{x_j}} \quad \text{for } i = 1, 2, \ldots, K
$$

![alt text](images/image-1.png)

不足：  
- 零点不可微
- 负输入梯度为0
