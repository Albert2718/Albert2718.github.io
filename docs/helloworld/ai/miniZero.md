<!-- ## AlphaZero

self-play 需要加入 Dirichlet noise 来增加探索性，从而避免过拟合。以及选择+对称产生共8种等价数据，减少自我对弈时间。 -->

## 知识背景

### 蒙特卡洛方法

>蒙特卡洛方法(Monte Carlo method)，是一种“统计模拟方法”。20世纪40年代，为建造核武器，冯.诺伊曼 等人发明了该算法。因赌城蒙特卡洛而得名，暗示其以概率作为算法的基础。  
>假设我们要计算一个不规则形状的面积，我们只需在包含这个不规则形状的矩形内，随机的掷出一个点，每掷出一个点，则N+1，如果这个点在不规则图形内则W+1。落入不规则图形的概率即为 W/N。当掷出足够多的点之后，我们可以认为：不规则图形面积＝矩形面积＊W/N。

### 蒙特卡洛树搜索(MCTS)

>1987年Bruce Abramson在他的博士论文中提出了基于蒙特卡洛方法的树搜索这一想法。这种算法简而言之是用蒙特卡洛方法估算每一种走法的胜率。如果描述的再具体一些，通过不断的模拟每一种走法，直至终局，该走法的模拟总次数N，与胜局次数W，即可推算出该走法的胜率为 W/N。通过随机的对游戏进行推演来逐渐建立一棵不对称的搜索树的过程。

一些写的很好的文章，都介绍了基础的MCTS：  

- [【最佳实战】蒙特卡洛树搜索算法](https://zhuanlan.zhihu.com/p/333348915)  
- [基于MCTS的黑白棋ai](https://www.sunnycandy.fun/post/other/Othello/)  

直观地说，我们平时下棋有时也会考虑我们下了这步棋后，对手**大概率**会怎么下，这样下后的局面又会如何，我们总不能臆想对手下一步非常不好的棋。此外，这里涉及到非常重要的一个概念：**视角**。

当我们有多个落子位置可选时，当然会希望选择胜率最大的位置，同样，对手也会希望选择他胜率最大的位置。首要的问题是，我们该如何确定这个胜率？

蒙特卡洛的核心思想是概率，用模拟来推断实际，我们可以在当前的棋盘状态下进行一定数量的模拟对局(rollout)，用他们终局的胜负或者其他合理的评判标准（如黑白棋的棋子数差）来给出当前节点的先验胜率or先验价值。

为了让模拟更有意义，我们需要有方向的模拟，也就是在下一步中，在对方的视角下，我们希望模拟对方胜率较高的落子，重要的是，对这些落子模拟的结果，也会实时的更新父节点的胜率/价值上，这是MCTS全局和局部结合的关键：只要树探索到了底部（叶子），就开始rollout模拟，然后将结果回传给上层节点，更新这些节点的胜率/平均价值估计。最终在当前需要落子的节点下，给出胜率/平均价值最高的落子。

*还有一些细节需要注意：实际上在选择节点模拟时不单单要看胜率，还要考虑探索性exploration，所以说往往在选择阶段MCTS都要用到UCT公式，这个公式综合考虑了胜率和探索性，从而在选择节点时，既能选择胜率高的节点，也能选择一些访问次数较少的节点进行探索。*

>提取并总结一下，选择阶段的作用是：选择出一条未来更可能发生的路径。
>而拓展、模拟、反向传播三个阶段则是通过这条路径，对第一级子节点的胜率进行调整，参入对敌我动向的分析，使该胜率更可信。

### MCTS with NN

TBH, 基础的MCTS在这次的黑白棋中已经表现的很厉害了，在模拟400步时，我和它下棋就已经很吃力了（当然我水平也有限，甚至可以说和他下棋我也学到了很多）。在mo平台的测试中，基础的mcts在模拟1000步的情况下也是顺利击败了高级玩家。

![alt text](images/image-1.png)


但是，总觉得它还不够智能，虽然MCTS的算法非常美妙，也并非全部依靠rollout来评估局面，但一是耗时较多，二是通过模拟很多局来获得所谓局面评估，这与人类的“棋感”还是有差距的。于是乎，神经网络可以登场了：通过输入此时棋盘的状态，输出所有的合法动作进行量化的评估价值，选择价值高的落子，这是我开始的直观想法，后来阅读材料后发现这叫raw network并且当时实际上AlphaGo也尝试用了raw network，效果也很好，不过震惊世界的AlphaGo Zero的核心算法其实是把神经网络和MCTS结合起来，MCTS可以被理解为一个强有力的策略改善的过程。

普通的MCTS在到达叶节点时，进行rollout模拟来评估局面，而结合神经网络的MCTS则是用神经网络来评估局面，从而大大提升了评估的效率。具体来说，神经网络会输出两个值：一个是策略向量，表示每个合法动作的概率分布；另一个是价值标量，表示当前局面的期望结果。在选择叶节点时，用到类似于UCT的评判公式PUCT：
$$
a_t = \arg\max_a \left( Q(s,a) + c_{puct} \cdot P(s,a) \cdot \frac{\sqrt{N(s)}}{1 + N(s,a)} \right)
$$ 
![alt text](images/image-2.png)，同样是平衡了探索性和利用性。

而神经网络的训练数据，则是来源于自我对弈产生的三元组 (s_t, π_t, z)，其中 s_t 是当前局面，π_t 是MCTS搜索得到的动作分布，z 是最终的游戏结果。训练的损失函数包括价值损失、策略损失和L2正则化:
$$
L = (z - v)^2 - \boldsymbol{\pi}^\top \log \mathbf{p} + c\|\theta\|^2
$$ 

在深入到技术细节之前，最后再用我的理解表达一下MCTS with NN究竟是怎么工作的：

首先是自对弈收集数据到数据(s, π, z)三元组，s表示当前的局面状态，π表示用当前的网络进行mcts后得到的各个动作的访问概率分布，z表示最终的胜负结果。

然后把这些数据放到神经网络进行训练，开始神经网络是随机初始化的，所以此时的价值和策略都是在随机预测，那么模拟出来的对局基本等同于随机对局（有一点rollout的感觉），**但是由于胜负结果是客观存在的**，模型在价值的预测上会慢慢准确，从而在选择阶段也会慢慢多选那些终局是胜利（价值高）的情况，同时也会促使模型在策略上学习（因为输出的策略不再那么的随机），于是给出的先验概率越来越具有参考意义。整体来看，这是一个不断循环，相辅相成的过程。


## 技术细节

### 自对弈




假设一盘棋走了 5 步，黑棋(X)赢了：


| 步骤 | 执行者 | 状态 | MCTS概率分布 | 最终结果 |
|------|--------|------|--------------|----------|
| 1    | X      | s₁   | π₁           | z=+1     |
| 2    | O      | s₂   | π₂           | z=-1     |
| 3    | X      | s₃   | π₃           | z=+1     |
| 4    | O      | s₄   | π₄           | z=-1     |
| 5    | X      | s₅   | π₅           | z=+1     |

> 共享同一盘棋的结果

原始数据: 5 个三元组  
增强后:   5 × 8 = 40 个训练样本









